{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# AI101 – Assignment 2: Finance Data Analysis\n",
    "**Dataset:** `finance_data.csv`  \n",
    "This notebook explores a financial dataset to understand its structure, clean it, and derive basic statistical insights."
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-imports",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Importing Libraries\n",
    "\n",
    "Before we can analyze any data, we need to import the Python libraries that provide the tools we'll use throughout this notebook.\n",
    "\n",
    "- **`pandas` (imported as `pd`)**: The primary library for data manipulation. It introduces the `DataFrame` — a table-like structure that makes reading, filtering, and transforming data easy.\n",
    "- **`numpy` (imported as `np`)**: A numerical computing library. It provides fast math operations, array support, and special values like `NaN` (Not a Number), which is used to represent missing data.\n",
    "- **`matplotlib.pyplot` (imported as `plt`)**: A plotting library that allows us to create charts and graphs to visualize data trends.\n",
    "- **`seaborn` (imported as `sns`)**: Built on top of `matplotlib`, seaborn makes it easier to create attractive statistical visualizations with less code.\n",
    "\n",
    "The `%matplotlib inline` magic command tells Jupyter to display any plots directly inside the notebook, rather than opening a separate window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-load",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Loading the Dataset\n",
    "\n",
    "Here we load the `finance_data.csv` file into a pandas `DataFrame` called `df`. A DataFrame is essentially a table with named columns and indexed rows — think of it as a spreadsheet inside Python.\n",
    "\n",
    "- **`pd.read_csv('finance_data.csv')`**: Reads the CSV (Comma-Separated Values) file from the current working directory and parses it into a DataFrame. Each column in the CSV becomes a column in the DataFrame.\n",
    "- **`df.head()`**: Displays the **first 5 rows** of the DataFrame. This is the standard first step when exploring a new dataset — it lets us quickly see the column names, data types, and what actual values look like. By default, `head()` shows 5 rows; you can pass a number (e.g., `df.head(10)`) to see more.\n",
    "\n",
    "**What the output represents:** A table showing the first five records of the finance dataset. This gives us a preview of the columns available (e.g., dates, amounts, categories) and helps us understand what kind of data we're working with before we dive deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('finance_data.csv')\n",
    "df.head()"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-shape",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Exploring the Shape and Structure\n",
    "\n",
    "Now we examine the overall size and structure of our dataset before doing any analysis.\n",
    "\n",
    "- **`df.shape`**: Returns a tuple `(rows, columns)`. For example, `(50000, 8)` means the dataset has 50,000 records and 8 columns. This tells us the scale of the data we're working with.\n",
    "- **`df.dtypes`**: Shows the **data type** of each column — for example, `int64` (whole numbers), `float64` (decimal numbers), or `object` (text/string). Knowing data types is critical because some operations only work on specific types (e.g., you cannot compute an average on a text column).\n",
    "- **`df.info()`**: Provides a concise summary of the entire DataFrame: the number of rows, each column name, how many non-null (non-missing) values exist in each column, and the data type. This is extremely useful for spotting missing data and confirming everything loaded correctly.\n",
    "\n",
    "**What the output represents:** A structural overview of the dataset. It tells us how large the data is, what each column contains, and whether any columns have missing values that need to be addressed during cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-missing",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Checking for Missing Values\n",
    "\n",
    "Real-world datasets almost always have missing or null values. Before analyzing data, we must understand how much is missing and decide how to handle it.\n",
    "\n",
    "- **`df.isnull()`**: Returns a DataFrame of the same shape as `df`, where each cell contains `True` if the value is missing (`NaN`) and `False` otherwise.\n",
    "- **`.sum()`**: Counts the number of `True` values in each column, giving us the **total number of missing values per column**.\n",
    "- **`df.isnull().sum() / len(df) * 100`**: Calculates the **percentage** of missing values in each column. Dividing the missing count by the total number of rows (`len(df)`) and multiplying by 100 converts it to a percentage. This helps us decide whether to drop a column (e.g., if 80% is missing) or fill it in.\n",
    "\n",
    "**What the output represents:** A column-by-column count and percentage of missing data. For example, if the `income` column shows `1500 (3.0%)`, it means 1,500 rows are missing an income value, which is 3% of the total dataset. We use this to decide our data cleaning strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.isnull().sum()\n",
    "missing_pct = df.isnull().sum() / len(df) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({'Missing Count': missing, 'Missing %': missing_pct})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-clean",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Cleaning the Data\n",
    "\n",
    "Data cleaning ensures our dataset is consistent and ready for analysis. Raw data often contains errors, duplicates, and missing values.\n",
    "\n",
    "- **`df.drop_duplicates(inplace=True)`**: Removes any rows that are **exact duplicates** of another row. The `inplace=True` argument modifies the DataFrame directly instead of returning a new one. Duplicate records can skew analysis results — for example, a transaction counted twice would inflate totals.\n",
    "- **`df.dropna(subset=[...])`** *(if used)*: Drops rows where **specific critical columns** are missing. We only drop rows where essential columns are null — columns that are less critical might be filled in instead.\n",
    "- **`df.fillna(df.median())`** or **`df[col].fillna(df[col].mean())`**: Fills missing numerical values with the **median** or **mean** of that column. The median is often preferred over the mean because it is less sensitive to extreme outliers (e.g., one very large transaction wouldn't distort the median as much).\n",
    "- **`df.reset_index(drop=True)`**: After removing rows, the index numbers may have gaps (e.g., 0, 1, 4, 5...). This resets them to a clean sequential index (0, 1, 2, 3...). `drop=True` prevents the old index from being added as a column.\n",
    "\n",
    "**What the output represents:** Confirmation of how many rows/columns remain after cleaning, and that the data is now free of duplicates and handled missing values — ready for reliable analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-clean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Fill missing numeric values with column median\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "\n",
    "# Reset index after dropping rows\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Cleaned dataset shape:\", df.shape)\n",
    "print(\"Remaining missing values:\", df.isnull().sum().sum())"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-describe",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Descriptive Statistics\n",
    "\n",
    "Descriptive statistics give us a high-level numerical summary of the dataset, helping us understand the distribution and spread of values.\n",
    "\n",
    "- **`df.describe()`**: Automatically calculates summary statistics for every **numerical column** in the DataFrame. The statistics it returns include:\n",
    "  - **`count`**: How many non-missing values exist in the column.\n",
    "  - **`mean`**: The average value across all rows.\n",
    "  - **`std`** (standard deviation): How spread out the values are around the mean. A high std means values vary a lot; a low std means they cluster tightly around the mean.\n",
    "  - **`min`** / **`max`**: The smallest and largest values in the column.\n",
    "  - **`25%`, `50%`, `75%`** (quartiles): These divide the data into four equal parts. The `50%` value is the **median** — the middle value. The gap between `25%` and `75%` (the interquartile range) tells us where the middle 50% of data lives.\n",
    "\n",
    "**What the output represents:** A comprehensive statistical snapshot of every numeric column. For a finance dataset, this might show the average transaction amount, the range of income values, and whether there are extreme outliers pulling the mean away from the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-describe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-viz1",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Distribution of a Key Numeric Variable\n",
    "\n",
    "Visualizing the distribution of a key variable helps us understand how values are spread — whether they're normally distributed, skewed, or contain outliers.\n",
    "\n",
    "- **`plt.figure(figsize=(10, 5))`**: Creates a new figure (canvas) for the plot, with a width of 10 inches and height of 5 inches. Setting the figure size ensures the chart is large and readable.\n",
    "- **`sns.histplot(df[col], kde=True, bins=40, color='steelblue')`**: Draws a **histogram** of a numeric column.\n",
    "  - A histogram groups values into \"bins\" (ranges) and counts how many values fall in each bin, showing the frequency distribution.\n",
    "  - **`kde=True`**: Overlays a **Kernel Density Estimate** curve — a smooth line that estimates the probability distribution of the data. It's like a smoothed version of the histogram.\n",
    "  - **`bins=40`**: The data is split into 40 equal-width bins. More bins = more detail; fewer bins = broader overview.\n",
    "- **`plt.title(...)`, `plt.xlabel(...)`, `plt.ylabel(...)`**: Add a title and axis labels to make the chart self-explanatory.\n",
    "- **`plt.show()`**: Renders and displays the completed plot.\n",
    "\n",
    "**What the output represents:** A histogram showing how frequently different value ranges appear in the data. For example, if plotting income, a right-skewed distribution would indicate most people have lower incomes with a few very high earners — which is typical of real-world finance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-viz1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of the first numeric column\n",
    "num_col = numeric_cols[0]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df[num_col], kde=True, bins=40, color='steelblue')\n",
    "plt.title(f'Distribution of {num_col}')\n",
    "plt.xlabel(num_col)\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-corr",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Correlation Heatmap\n",
    "\n",
    "A correlation heatmap shows us how strongly pairs of numeric variables are related to each other. This is a foundational step before building any predictive model.\n",
    "\n",
    "- **`df.corr()`**: Computes the **Pearson correlation coefficient** between every pair of numeric columns. The result is a matrix where each cell contains a value between **-1 and 1**:\n",
    "  - **1.0**: Perfect positive correlation (as one variable increases, the other increases proportionally).\n",
    "  - **-1.0**: Perfect negative correlation (as one increases, the other decreases).\n",
    "  - **0**: No linear relationship.\n",
    "  - Values closer to ±1 indicate stronger relationships.\n",
    "- **`sns.heatmap(..., annot=True, cmap='coolwarm', fmt='.2f')`**: Visualizes the correlation matrix as a color-coded grid.\n",
    "  - **`annot=True`**: Displays the actual correlation number inside each cell.\n",
    "  - **`cmap='coolwarm'`**: Uses a red-blue color scale — red indicates positive correlation, blue indicates negative.\n",
    "  - **`fmt='.2f'`**: Formats the numbers to 2 decimal places for readability.\n",
    "  - **`linewidths=0.5`**: Adds thin lines between cells to improve readability.\n",
    "\n",
    "**What the output represents:** A grid of color-coded correlation values. In a finance dataset, for example, a high positive correlation between `credit_score` and `loan_approval` would suggest that higher credit scores are associated with higher approval rates — a relationship worth investigating further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-corr",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f',\n",
    "            linewidths=0.5, square=True)\n",
    "plt.title('Correlation Heatmap of Numeric Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-categorical",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Analyzing Categorical Variables\n",
    "\n",
    "Not all columns contain numbers. Categorical columns contain text labels (e.g., \"Male/Female\", \"Yes/No\", \"City names\"). Here we explore the distribution of those categories.\n",
    "\n",
    "- **`df.select_dtypes(include='object').columns`**: Selects only columns that have the `object` data type, which in pandas typically means **text/string columns** (i.e., categorical data).\n",
    "- **`df[col].value_counts()`**: For a given column, counts how many times each unique value appears, and returns them in **descending order** (most common first). This is the fundamental way to understand the composition of a categorical variable.\n",
    "- **`sns.countplot(...)`**: Draws a bar chart where each bar represents a category and its height represents the count. This makes it visually easy to compare the frequency of each category.\n",
    "- **`plt.xticks(rotation=45)`**: Rotates the x-axis labels by 45 degrees so that longer category names don't overlap and remain readable.\n",
    "\n",
    "**What the output represents:** Bar charts for categorical columns showing how the data is distributed across categories. For example, in a finance dataset a `loan_purpose` column might show that most loans are for \"debt consolidation\" while very few are for \"education\" — this kind of insight informs further targeted analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-categorical",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include='object').columns\n",
    "print(f\"Categorical columns: {list(cat_cols)}\\n\")\n",
    "\n",
    "for col in cat_cols[:3]:  # Show up to 3 categorical columns\n",
    "    print(f\"--- {col} ---\")\n",
    "    print(df[col].value_counts(), \"\\n\")\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    order = df[col].value_counts().index[:15]  # Top 15 categories\n",
    "    sns.countplot(data=df, x=col, order=order, palette='viridis')\n",
    "    plt.title(f'Value Counts: {col}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-boxplot",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Outlier Detection with Box Plots\n",
    "\n",
    "Outliers are data points that are unusually far from the rest of the data. In finance, outliers could represent fraud, data entry errors, or genuinely unusual transactions. Box plots are a standard tool for spotting them.\n",
    "\n",
    "- **Box plot anatomy**: A box plot shows five key statistics:\n",
    "  - The **box** spans from the **25th percentile (Q1)** to the **75th percentile (Q3)** — this is the Interquartile Range (IQR), containing the middle 50% of data.\n",
    "  - The **line inside the box** is the **median (50th percentile)**.\n",
    "  - The **whiskers** extend to the furthest values within 1.5× the IQR from Q1 and Q3.\n",
    "  - Any **dots beyond the whiskers** are classified as **outliers**.\n",
    "- **`sns.boxplot(data=df, y=col)`**: Creates a vertical box plot for a numeric column.\n",
    "- **Why this matters**: If the `amount` column has many extreme outliers, they could skew mean-based calculations and mislead predictive models. Outliers need to be investigated — removed, capped, or treated as a separate segment.\n",
    "\n",
    "**What the output represents:** A visual summary of a column's distribution and any extreme values. Dots appearing far above the top whisker (or below the bottom) are outliers. In a finance dataset, a single extremely high transaction amount appearing as an outlier should be investigated — is it legitimate, or an error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-boxplot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, min(3, len(numeric_cols)), figsize=(14, 5))\n",
    "\n",
    "if len(numeric_cols) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, col in zip(axes, numeric_cols[:3]):\n",
    "    sns.boxplot(data=df, y=col, ax=ax, color='lightcoral')\n",
    "    ax.set_title(f'Box Plot: {col}')\n",
    "\n",
    "plt.suptitle('Outlier Detection via Box Plots', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },

  {
   "cell_type": "markdown",
   "id": "md-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: Summary and Key Takeaways\n",
    "\n",
    "This final section summarizes what we learned from the finance dataset analysis.\n",
    "\n",
    "- **`df.shape`**: We print the final cleaned shape of the DataFrame to confirm how many rows and columns remain after preprocessing.\n",
    "- **`df.describe().T`**: We transpose (`.T`) the descriptive statistics table so that columns become rows — this makes it easier to read when there are many columns, as all stats for one variable are on one line.\n",
    "- **Key insights recorded below**: Based on the visualizations and statistics above, we document the main findings — things like which variables have the widest spread, what the most common categories are, and which pairs of variables are most correlated.\n",
    "\n",
    "**What the output represents:** A final clean summary of the dataset's key statistical properties after cleaning and exploration. This serves as the foundation for the next phase of the project — feature engineering and building a predictive model with the more complex dataset in Assignment 2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Records: {df.shape[0]:,}\")\n",
    "print(f\"Total Features: {df.shape[1]}\")\n",
    "print(f\"Numeric Features: {len(numeric_cols)}\")\n",
    "print(f\"Categorical Features: {len(cat_cols)}\")\n",
    "print(f\"Missing Values Remaining: {df.isnull().sum().sum()}\")\n",
    "print(\"\\nDescriptive Statistics (transposed):\")\n",
    "df.describe().T"
   ]
  }
 ]
}
